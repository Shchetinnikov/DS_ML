{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Логистическая регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Логистическая регрессия(Logistic regression)__ - метод построения линейного классификатора, позволяющий оценивать апостериорные(вычисляемые) вероятности принадлежности объектов к классам."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Несмотря на наличие слова \"регрессия\" в названии, логистическая регрессия на самом деле является широко спользуемым бинарным классификатором(т.е. вектор целей может принимать только два значения). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__В методе логистической регрессии не производится предсказание значения числовой переменной исходя из выборки исходных значений. Вместо этого, значением функции является вероятность того, что данное исходное значение принадлежит к определенному классу.__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Присказка$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Предположим, что есть абонент,который выбирает купить или не купить услугу. Он покупает услугу, если \"польза\" от услуги больше нуля. Запишем:$$y^*_i=x^`_iw+\\xi_i$$\n",
    "- Модель скрытой переменной, где $y^*_i$ - ненаблюдаемая переменная, а $\\xi_i$ - ошибка, имеющая распределение F. \n",
    "$$y_i=1,   если   y^*_i>=0$$\n",
    "$$y_i=0,      otherwise$$\n",
    "Тогда:\n",
    "$$\\text P\\left(y_i = 1 \\mid x_i\\right) = \\text P\\left(y^*_i >= 0 \\mid x_i\\right) = \\text P\\left(x^`_iw+\\xi_i >= 0 \\mid x_i\\right) = \\text P\\left(\\xi_i >= -x^`_iw \\mid x_i\\right) = 1 - F(-x^`_iw) = F(x^`_iw) $$\n",
    "- Предположим, распределение ошибки - логистическое. То есть:\n",
    "$$F(\\xi)=\\frac{1}{1 + \\exp^{-\\xi}}$$\n",
    "- Отсюда получаем логистическую регрессию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Линейный классификатор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основная идея линейного классификатора заключается в том, что признаковое пространство может быть разделено гиперплоскостью на две полуплоскости, в каждой из которых прогнозируется одно из двух значений целевого класса. \n",
    "Если это можно сделать без ошибок, то обучающая выборка называется *линейно разделимой*.\n",
    "\n",
    "<img src=\"./logit.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В логистической регрессии линейная модель (например  $\\beta_0$+$\\beta_1$x) включается в логистическую(так называемую сигмоидальную) функцию $\\frac{1}{1 + \\exp^{-\\textbf{z}}}$ таким образом, что:\n",
    "$$\\text P\\left(y_i = 1 \\mid \\text{X}\\right) = \\frac{1}{1 + \\exp^{-(\\beta_0+\\beta_1 x)}} $$\n",
    "где $\\text P\\left(y_i = 1 \\mid \\text{X}\\right)$ - вероятность, что целевое значение i-го наблюдения $y_i$ является классом 1;\n",
    "$Х$ - тренировочные данные; $\\beta_0$ и $\\beta_1$ - параметры, которые необходимо заучить; $e$ - эйлерово число. Эффект логистической функции заключается в ограничении значения результата функции диапазоном между 0 и 1, чтобы его можно было интепретировать как вероятность. Если $\\text P\\left(y_i = 1 \\mid \\text{X}\\right)$ больше 0.5, то предсказывается класс 1, в противном случае - класс 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Логистическая регрессия как линейный классификатор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Логистическая регрессия является частным случаем линейного классификатора, но она обладает хорошим \"умением\" – прогнозировать вероятность $p_+$ отнесения вектора признаков $\\vec{x_i}$ к классу \"+\":"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, мы хотим прогнозировать вероятность $p_+ \\in [0,1]$, а пока умеем строить линейный прогноз с помощью МНК: $b(\\textbf{x}) = \\textbf{w}^\\text{T} \\textbf{x} \\in \\mathbb{R}$. Каким образом преобразовать полученное значение в вероятность, пределы которой – [0, 1]? Очевидно, для этого нужна некоторая функция $f: \\mathbb{R} \\rightarrow [0,1].$ В модели логистической регрессии для этого берется конкретная функция: $\\sigma(z) = \\frac{1}{1 + \\exp^{-z}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обозначим $P(X)$ вероятностью происходящего события $X$. Тогда отношение вероятностей $OR(X)$ определяется из $\\frac{P(X)}{1-P(X)}$, а это — отношение вероятностей того, произойдет ли событие или не произойдет. Очевидно, что вероятность и отношение шансов содержат одинаковую информацию. Но в то время как $P(X)$ находится в пределах от 0 до 1, $OR(X)$ находится в пределах от 0 до $\\infty$.\n",
    "\n",
    "Если вычислить логарифм $OR(X)$ (то есть называется логарифм шансов, или логарифм отношения вероятностей), то легко заметить, что $\\log{OR(X)} \\in \\mathbb{R}$. Его то мы и будем прогнозировать с помощью МНК.\n",
    "\n",
    "Посмотрим, как логистическая регрессия будет делать прогноз $p_+ = \\text{P}\\left(y_i = 1 \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right)$ (пока считаем, что веса $\\textbf{w}$ мы как-то получили (т.е. обучили модель), далее разберемся, как именно). \n",
    "\n",
    "**Шаг 1.** Вычислить значение $w_{0}+w_{1}x_1 + w_{2}x_2 + ... = \\textbf{w}^\\text{T}\\textbf{x}$. (уравнение $\\textbf{w}^\\text{T}\\textbf{x} = 0$ задает гиперплоскость, разделяющую примеры на 2 класса);\n",
    "\n",
    "\n",
    "**Шаг 2.** Вычислить логарифм отношения шансов: $ \\log(OR_{+}) =  \\textbf{w}^\\text{T}\\textbf{x}$.\n",
    "\n",
    "**Шаг 3.** Имея прогноз шансов на отнесение к классу \"+\" – $OR_{+}$, вычислить $p_{+}$ с помощью простой зависимости:\n",
    "\n",
    "$$\\Large p_{+} = \\frac{OR_{+}}{1 + OR_{+}} = \\frac{\\exp^{\\textbf{w}^\\text{T}\\textbf{x}}}{1 + \\exp^{\\textbf{w}^\\text{T}\\textbf{x}}} =  \\frac{1}{1 + \\exp^{-\\textbf{w}^\\text{T}\\textbf{x}}} = \\sigma(\\textbf{w}^\\text{T}\\textbf{x})$$\n",
    "\n",
    "\n",
    "В правой части мы получили как раз сигмоид-функцию.\n",
    "\n",
    "Итак, логистическая регрессия прогнозирует вероятность отнесения примера к классу \"+\" (при условии, что мы знаем его признаки и веса модели) как сигмоид-преобразование линейной комбинации вектора весов модели и вектора признаков примера:\n",
    "\n",
    "$$\\Large p_+(x_i) = \\text{P}\\left(y_i = 1 \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) = \\sigma(\\textbf{w}^\\text{T}\\textbf{x}_\\text{i}). $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хотим попробовать применить ту же модель линейной регрессии, но для задачи классификации. То есть\n",
    "$$\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_k x_{ik} = y_i,$$\n",
    "где $X_i = (x_{i1}, \\ldots, x_{ik})$ -- строка с данными, $y_i\\in\\{0, 1\\}$, $\\beta_0, \\ldots, \\beta_k$ -- неизвестные коэффициенты.\n",
    "Проблема в том, что слева стоит непрерывная функция по $x$, а справа -- дискретная переменная. Кроме того, при больших значениях $x$ функция слева будет гарантированно давать большие значения. Так что применить модель прямо в таком виде -- плохая идея.\n",
    "\n",
    "Введём логистическую функцию (сигмоид) $\\sigma(x)=\\frac{1}{1+e^{-x}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x=np.array(np.arange(-10,10,0.5))\n",
    "y=1./(1+np.exp(-x))\n",
    "plt.plot(x,y)\n",
    "plt.title(u'Логистическая функция');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметим, что $\\sigma: \\mathbb{R}\\rightarrow (0, 1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим к левой части логистическую функцию и будем решать задачу в виде\n",
    "$$\\sigma\\left(\\beta_0 + \\beta_1 x_{i1} + \\ldots + \\beta_k x_{ik}\\right) = y_i.$$\n",
    "\n",
    "Теперь слева у нас всегда будут получаться значения из $(0, 1)$, и это можно интерпретировать как вероятности принадлежности классу: если меньше 0.5, то класс 0, иначе -- класс 1.\n",
    "\n",
    "Но теперь у нас пропадает аналитическое решение, которое было для линейной регрессии. Теперь придётся решать задачу оптимизации. Градиентный спуск (или другие методы) в помощь!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логистическая модель в sklearn реализована в классе ```LogisticRegression``` библиотеки ```linear_model```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(\n",
    "    # метод для поиска решения. Для небольших датасетов лучше подходит liblinear, sag и saga -- для больших.\n",
    "    # Варианты: newton-cg, lbfgs, liblinear, sag, saga\n",
    "    solver='liblinear',\n",
    "    # норма для регуляризации. Варианты: l2, l1.\n",
    "    penalty='l2',\n",
    "    # параметр регуляризации. Чем меньше, тем сильнее регуляризация. Положительный.\n",
    "    C=1,\n",
    "    # параметр для остановки поиска решения.\n",
    "    tol=1e-4,\n",
    "    # Так как распознается 2 класса.\n",
    "    multi_class='ovr'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Натренировать логистическую регрессию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Загрузим данные только с двумя классами\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data[:100,:]\n",
    "target = iris.target[:100]\n",
    "\n",
    "#Стандартизировать признаки\n",
    "scaler = StandardScaler()\n",
    "features_standardized = scaler.fit_transform(features)\n",
    "\n",
    "#Создать объект логистической регрессии\n",
    "logistic_regression = LogisticRegression(random_state=0)\n",
    "\n",
    "#Натренировать модель\n",
    "model = logistic_regression.fit(features_standardized,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Создать новое наблюдение\n",
    "new_observation = [[.5, .5, .5, .5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Предсказать класс\n",
    "model.predict(new_observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом примере наше наблюдение было предсказано как класс 1. Кроме того, мы можем увидеть вероятность принадлежности наблюдения к каждому классу:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.39173187, 0.60826813]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(new_observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Принцип максимального правдоподобия и логистическая регрессия\n",
    "Теперь посмотрим, как из принципа максимального правдоподобия получается оптимизационная задача, которую решает логистическая регрессия, а именно, – минимизация *логистической* функции потерь. \n",
    "Только что мы увидели, что логистическая регрессия моделирует вероятность отнесения примера к классу \"+\" как \n",
    "\n",
    "$$\\Large p_+(\\textbf{x}_\\text{i}) = \\text{P}\\left(y_i = 1 \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) = \\sigma(\\textbf{w}^\\text{T}\\textbf{x}_\\text{i})$$\n",
    "\n",
    "Тогда для класса \"-\" аналогичная вероятность:\n",
    "$$\\Large p_-(\\textbf{x}_\\text{i})  = \\text{P}\\left(y_i = -1 \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right)  = 1 - \\sigma(\\textbf{w}^\\text{T}\\textbf{x}_\\text{i}) = \\sigma(-\\textbf{w}^\\text{T}\\textbf{x}_\\text{i}) $$\n",
    "\n",
    "Оба этих выражения можно  объединить в одно:\n",
    "\n",
    "$$\\Large \\text{P}\\left(y = y_i \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) = \\sigma(y_i\\textbf{w}^\\text{T}\\textbf{x}_\\text{i})$$\n",
    "\n",
    "Выражение $M(\\textbf{x}_\\text{i}) = y_i\\textbf{w}^\\text{T}\\textbf{x}_\\text{i}$ называется *отступом* (*margin*) классификации на объекте $\\textbf{x}_\\text{i}$ (не путать с зазором (тоже margin), про который чаще всего говорят в контексте SVM). Если он неотрицателен, модель не ошибается на объекте $\\textbf{x}_\\text{i}$, если же отрицателен – значит, класс для $\\textbf{x}_\\text{i}$  спрогнозирован неправильно. \n",
    "Заметим, что отступ определен для объектов именно обучающей выборки, для которых известны реальные метки целевого класса $y_i$.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь распишем правдоподобие выборки, а именно, вероятность наблюдать данный вектор $\\textbf{y}$ у выборки $\\textbf X$. Делаем сильное предположение: объекты приходят независимо, из одного распределения (*i.i.d.*). Тогда\n",
    "\n",
    "$$\\Large \\text{P}\\left(\\textbf{y} \\mid \\textbf X, \\textbf{w}\\right) = \\prod_{i=1}^{\\ell} \\text{P}\\left(y = y_i \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right),$$\n",
    "\n",
    "где $\\ell$ – длина выборки $\\textbf X$ (число строк).\n",
    "\n",
    "Как водится, возьмем логарифм данного выражения (сумму оптимизировать намного проще, чем произведение):\n",
    "\n",
    "$$\\Large  \\log \\text{P}\\left(\\textbf{y} \\mid \\textbf X, \\textbf{w}\\right) = \\log \\sum_{i=1}^{\\ell} \\text{P}\\left(y = y_i \\mid \\textbf{x}_\\text{i}, \\textbf{w}\\right) = \\log \\prod_{i=1}^{\\ell} \\sigma(y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i})   = $$\n",
    "\n",
    "$$\\Large  = \\sum_{i=1}^{\\ell} \\log \\sigma(y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}) = \\sum_{i=1}^{\\ell} \\log \\frac{1}{1 + \\exp^{-y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}}} = - \\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть в данном случае принцип максимизации правдоподобия приводит к минимизации выражения \n",
    "\n",
    "$$\\Large \\mathcal{L_{log}} (\\textbf X, \\textbf{y}, \\textbf{w}) = \\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\textbf{w}^{\\text{T}}\\textbf{x}_\\text{i}}).$$\n",
    "\n",
    "Это *логистическая* функция потерь, просуммированная по всем объектам обучающей выборки.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
